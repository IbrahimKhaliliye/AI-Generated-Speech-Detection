{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Function to convert audio to waveform image\n",
    "def save_waveform_image(audio_path, save_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.plot(y)\n",
    "        plt.title('Waveform')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close('all')\n",
    "    except:\n",
    "        print('fail')\n",
    "        pass\n",
    "# Preprocess audio files to waveform images\n",
    "\n",
    "def preprocess_audio_batch(audio_files, audio_dir, image_dir):\n",
    "    for file in audio_files:\n",
    "        if file.endswith(('.wav', '.flac')):\n",
    "            audio_path = os.path.join(audio_dir, file)\n",
    "            relative_path = os.path.relpath(audio_path, audio_dir)\n",
    "            save_path = os.path.join(image_dir, relative_path)\n",
    "            \n",
    "            if file.endswith('.wav'):\n",
    "                save_path = save_path.replace('.wav', '.png')\n",
    "            elif file.endswith('.flac'):\n",
    "                save_path = save_path.replace('.flac', '.png')\n",
    "            \n",
    "            save_dir = os.path.dirname(save_path)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            save_waveform_image(audio_path, save_path)\n",
    "# Organize images into train and test directories\n",
    "def organize_images(image_dir, train_dir, test_dir, test_size=0.2):\n",
    "    categories = ['real2000', 'fake2000']\n",
    "    for category in categories:\n",
    "        category_dir = os.path.join(image_dir, category)\n",
    "        images = [os.path.join(category_dir, img) for img in os.listdir(category_dir) if img.endswith('.png')]\n",
    "        train_images, test_images = train_test_split(images, test_size=test_size)\n",
    "        \n",
    "        for img_set, set_dir in zip([train_images, test_images], [train_dir, test_dir]):\n",
    "            category_set_dir = os.path.join(set_dir, category)\n",
    "            if not os.path.exists(category_set_dir):\n",
    "                os.makedirs(category_set_dir)\n",
    "            for img_path in img_set:\n",
    "                shutil.copy(img_path, category_set_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_audio_dir =r'C:\\Users\\alisa\\Downloads\\10krealaudio'\n",
    "fake_audio_dir = r'C:\\Users\\alisa\\Downloads\\largerfake'\n",
    "real_image_dir = '10kimages/real2000'\n",
    "fake_image_dir = '10kimages/fake2000'\n",
    "image_dir = '10kimages'\n",
    "train_dir = '10kimages/train'\n",
    "test_dir = '10kimages/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files =[f for f in os.listdir(real_audio_dir) if f.endswith(('.wav', '.flac'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = all_files[:2000]\n",
    "batch2 = all_files[2001:4000]\n",
    "batch3 = all_files[4001:6000]\n",
    "batch4 = all_files[6001:8000]\n",
    "batch5 = all_files[8001:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alisa\\AppData\\Local\\Temp\\ipykernel_20816\\840311286.py:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_path, sr=None)\n",
      "C:\\Users\\alisa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n"
     ]
    }
   ],
   "source": [
    "preprocess_audio_batch(batch1, real_audio_dir, real_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(batch2, real_audio_dir, real_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(batch3, real_audio_dir, real_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(batch4, real_audio_dir, real_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(batch5, real_audio_dir, real_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_files =[f for f in os.listdir(fake_audio_dir) if f.endswith(('.wav', '.flac'))]\n",
    "fakebatch1 = fake_files[:2000]\n",
    "fakebatch2= fake_files[2001:4000]\n",
    "fakebatch3 = fake_files[4001:6000]\n",
    "fakebatch4 = fake_files[6001:8000]\n",
    "fakebatch5 = fake_files[8001:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(fakebatch1, fake_audio_dir, fake_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(fakebatch2, fake_audio_dir, fake_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(fakebatch3, fake_audio_dir, fake_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(fakebatch4, fake_audio_dir, fake_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio_batch(fakebatch5, fake_audio_dir, fake_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_images(image_dir, train_dir, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13569 images belonging to 2 classes.\n",
      "Found 2393 images belonging to 2 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alisa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m770s\u001b[0m 2s/step - accuracy: 0.7210 - loss: 0.8619 - val_accuracy: 0.9671 - val_loss: 0.1959\n",
      "Epoch 2/5\n",
      "\u001b[1m  1/424\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:28\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.3909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.3909 - val_accuracy: 1.0000 - val_loss: 0.1812\n",
      "Epoch 3/5\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 2s/step - accuracy: 0.8428 - loss: 0.3787 - val_accuracy: 0.9286 - val_loss: 0.2359\n",
      "Epoch 4/5\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.3931 - val_accuracy: 0.8800 - val_loss: 0.3005\n",
      "Epoch 5/5\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 2s/step - accuracy: 0.8589 - loss: 0.3445 - val_accuracy: 0.9768 - val_loss: 0.1714\n",
      "Epoch 1/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m782s\u001b[0m 2s/step - accuracy: 0.8746 - loss: 0.3185 - val_accuracy: 0.9481 - val_loss: 0.1979\n",
      "Epoch 2/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.2983 - val_accuracy: 0.9200 - val_loss: 0.2332\n",
      "Epoch 3/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 2s/step - accuracy: 0.8804 - loss: 0.2983 - val_accuracy: 0.9611 - val_loss: 0.1555\n",
      "Epoch 4/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.4093 - val_accuracy: 0.8800 - val_loss: 0.1842\n",
      "Epoch 5/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m781s\u001b[0m 2s/step - accuracy: 0.8890 - loss: 0.2779 - val_accuracy: 0.9633 - val_loss: 0.1616\n",
      "Epoch 6/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7188 - loss: 0.4896 - val_accuracy: 1.0000 - val_loss: 0.0891\n",
      "Epoch 7/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m783s\u001b[0m 2s/step - accuracy: 0.8958 - loss: 0.2605 - val_accuracy: 0.9742 - val_loss: 0.1425\n",
      "Epoch 8/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.3179 - val_accuracy: 1.0000 - val_loss: 0.1187\n",
      "Epoch 9/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m789s\u001b[0m 2s/step - accuracy: 0.8963 - loss: 0.2528 - val_accuracy: 0.9785 - val_loss: 0.1165\n",
      "Epoch 10/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8438 - loss: 0.3982 - val_accuracy: 0.9600 - val_loss: 0.1768\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary',\n",
    "                                                    subset='training')\n",
    "validation_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                         target_size=(224, 224),\n",
    "                                                         batch_size=32,\n",
    "                                                         class_mode='binary',\n",
    "                                                         subset='validation')\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "                    validation_steps=validation_generator.samples // validation_generator.batch_size)\n",
    "\n",
    "# Unfreeze some layers of the base model\n",
    "for layer in base_model.layers[-3:]:  # Unfreeze the last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # Lower learning rate\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "history_fine = model.fit(train_generator,\n",
    "                         validation_data=validation_generator,\n",
    "                         epochs=10,\n",
    "                         steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "                         validation_steps=validation_generator.samples // validation_generator.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.996511]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = r'C:\\Users\\alisa\\Downloads\\record_out.wav'\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Plot and save the waveform image\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y)\n",
    "plt.title('Waveform')\n",
    "waveform_image_path = 'workplz.png'\n",
    "plt.savefig(waveform_image_path)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the image\n",
    "img = image.load_img(waveform_image_path, target_size=(224,224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Normalize the image\n",
    "img_array /= 255.0\n",
    "\n",
    "# Expand dimensions to match the input shape of the model\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "model.predict(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('10kimagesVGG16.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3992 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alisa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.8911 - loss: 0.2957\n",
      "Test Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='binary')\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13569 images belonging to 2 classes.\n",
      "Found 2393 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alisa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alisa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 211ms/step - accuracy: 0.6918 - loss: 0.6173 - val_accuracy: 0.8809 - val_loss: 0.3352\n",
      "Epoch 2/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - accuracy: 0.8438 - loss: 0.3444 - val_accuracy: 0.7600 - val_loss: 0.4486\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 206ms/step - accuracy: 0.8447 - loss: 0.3742 - val_accuracy: 0.8792 - val_loss: 0.3392\n",
      "Epoch 4/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - accuracy: 0.9062 - loss: 0.3424 - val_accuracy: 0.9600 - val_loss: 0.2820\n",
      "Epoch 5/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 206ms/step - accuracy: 0.8584 - loss: 0.3447 - val_accuracy: 0.8801 - val_loss: 0.2894\n",
      "Epoch 6/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - accuracy: 0.8750 - loss: 0.3614 - val_accuracy: 0.8400 - val_loss: 0.3280\n",
      "Epoch 7/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 206ms/step - accuracy: 0.8706 - loss: 0.3143 - val_accuracy: 0.8944 - val_loss: 0.3130\n",
      "Epoch 8/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201us/step - accuracy: 0.7500 - loss: 0.4193 - val_accuracy: 0.8800 - val_loss: 0.2673\n",
      "Epoch 9/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 207ms/step - accuracy: 0.8818 - loss: 0.2930 - val_accuracy: 0.8801 - val_loss: 0.3031\n",
      "Epoch 10/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step - accuracy: 0.8125 - loss: 0.3951 - val_accuracy: 0.8000 - val_loss: 0.3023\n",
      "Epoch 11/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 206ms/step - accuracy: 0.8851 - loss: 0.2809 - val_accuracy: 0.9105 - val_loss: 0.2528\n",
      "Epoch 12/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210us/step - accuracy: 0.9375 - loss: 0.2428 - val_accuracy: 0.9200 - val_loss: 0.1982\n",
      "Epoch 13/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 207ms/step - accuracy: 0.8943 - loss: 0.2546 - val_accuracy: 0.9012 - val_loss: 0.2629\n",
      "Epoch 14/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220us/step - accuracy: 0.8750 - loss: 0.3388 - val_accuracy: 0.8400 - val_loss: 0.2856\n",
      "Epoch 15/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 206ms/step - accuracy: 0.8998 - loss: 0.2297 - val_accuracy: 0.8961 - val_loss: 0.2831\n",
      "Epoch 16/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - accuracy: 0.8750 - loss: 0.2845 - val_accuracy: 0.9200 - val_loss: 0.2020\n",
      "Epoch 17/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 206ms/step - accuracy: 0.9144 - loss: 0.2040 - val_accuracy: 0.8944 - val_loss: 0.2615\n",
      "Epoch 18/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225us/step - accuracy: 0.9688 - loss: 0.1072 - val_accuracy: 0.8400 - val_loss: 0.2530\n",
      "Epoch 19/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 212ms/step - accuracy: 0.9317 - loss: 0.1679 - val_accuracy: 0.8965 - val_loss: 0.2962\n",
      "Epoch 20/20\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210us/step - accuracy: 0.9062 - loss: 0.2614 - val_accuracy: 0.9200 - val_loss: 0.2905\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(128, 128),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary',\n",
    "                                                    subset='training')\n",
    "validation_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                         target_size=(128, 128),\n",
    "                                                         batch_size=32,\n",
    "                                                         class_mode='binary',\n",
    "                                                         subset='validation')\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "                    validation_steps=validation_generator.samples // validation_generator.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('10kimagesCNN.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = r'C:\\Users\\alisa\\Downloads\\ali.wav'\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Plot and save the waveform image\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y)\n",
    "plt.title('Waveform')\n",
    "waveform_image_path = 'workplz.png'\n",
    "plt.savefig(waveform_image_path)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the image\n",
    "img = image.load_img(waveform_image_path, target_size=(128,128))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Normalize the image\n",
    "img_array /= 255.0\n",
    "\n",
    "# Expand dimensions to match the input shape of the model\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "model = tf.keras.models.load_model('10kimagesCNN.keras')\n",
    "\n",
    "model.predict(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3992 images belonging to 2 classes.\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 151ms/step - accuracy: 0.8618 - loss: 0.3545\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(128, 128),\n",
    "                                                  batch_size=32,\n",
    "                                                  class_mode='binary')\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13569 images belonging to 2 classes.\n",
      "Found 2393 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 2s/step - accuracy: 0.7466 - loss: 0.7860 - val_accuracy: 0.9519 - val_loss: 0.2021\n",
      "Epoch 2/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.4610 - val_accuracy: 0.9200 - val_loss: 0.2269\n",
      "Epoch 3/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 2s/step - accuracy: 0.8366 - loss: 0.3839 - val_accuracy: 0.9611 - val_loss: 0.1969\n",
      "Epoch 4/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.4179 - val_accuracy: 0.8800 - val_loss: 0.2009\n",
      "Epoch 5/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 2s/step - accuracy: 0.8513 - loss: 0.3614 - val_accuracy: 0.9721 - val_loss: 0.1620\n",
      "Epoch 6/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2849 - val_accuracy: 0.9600 - val_loss: 0.1985\n",
      "Epoch 7/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m896s\u001b[0m 2s/step - accuracy: 0.8529 - loss: 0.3593 - val_accuracy: 0.9569 - val_loss: 0.2086\n",
      "Epoch 8/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.4146 - val_accuracy: 0.9200 - val_loss: 0.2855\n",
      "Epoch 9/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m895s\u001b[0m 2s/step - accuracy: 0.8634 - loss: 0.3311 - val_accuracy: 0.9058 - val_loss: 0.2406\n",
      "Epoch 10/10\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.3589 - val_accuracy: 1.0000 - val_loss: 0.1481\n",
      "Epoch 1/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1017s\u001b[0m 2s/step - accuracy: 0.8752 - loss: 0.3032 - val_accuracy: 0.9666 - val_loss: 0.1486\n",
      "Epoch 2/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8750 - loss: 0.3921 - val_accuracy: 0.8800 - val_loss: 0.2891\n",
      "Epoch 3/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1015s\u001b[0m 2s/step - accuracy: 0.8878 - loss: 0.2714 - val_accuracy: 0.9780 - val_loss: 0.1332\n",
      "Epoch 4/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 0.2530 - val_accuracy: 1.0000 - val_loss: 0.0640\n",
      "Epoch 5/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 2s/step - accuracy: 0.8999 - loss: 0.2474 - val_accuracy: 0.9675 - val_loss: 0.1504\n",
      "Epoch 6/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1890 - val_accuracy: 1.0000 - val_loss: 0.0851\n",
      "Epoch 7/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1019s\u001b[0m 2s/step - accuracy: 0.9083 - loss: 0.2221 - val_accuracy: 0.9755 - val_loss: 0.1254\n",
      "Epoch 8/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9062 - loss: 0.2144 - val_accuracy: 0.9600 - val_loss: 0.1564\n",
      "Epoch 9/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1018s\u001b[0m 2s/step - accuracy: 0.9198 - loss: 0.2036 - val_accuracy: 0.9688 - val_loss: 0.1215\n",
      "Epoch 10/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.1705 - val_accuracy: 1.0000 - val_loss: 0.0907\n",
      "Epoch 11/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1016s\u001b[0m 2s/step - accuracy: 0.9333 - loss: 0.1760 - val_accuracy: 0.9139 - val_loss: 0.2044\n",
      "Epoch 12/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.2194 - val_accuracy: 0.8800 - val_loss: 0.2286\n",
      "Epoch 13/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1015s\u001b[0m 2s/step - accuracy: 0.9407 - loss: 0.1534 - val_accuracy: 0.9468 - val_loss: 0.1424\n",
      "Epoch 14/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1483 - val_accuracy: 1.0000 - val_loss: 0.0791\n",
      "Epoch 15/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 2s/step - accuracy: 0.9554 - loss: 0.1335 - val_accuracy: 0.9628 - val_loss: 0.1088\n",
      "Epoch 16/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.1253 - val_accuracy: 0.9600 - val_loss: 0.1623\n",
      "Epoch 17/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 2s/step - accuracy: 0.9612 - loss: 0.1140 - val_accuracy: 0.9696 - val_loss: 0.0980\n",
      "Epoch 18/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1522 - val_accuracy: 0.8800 - val_loss: 0.1337\n",
      "Epoch 19/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1013s\u001b[0m 2s/step - accuracy: 0.9740 - loss: 0.0922 - val_accuracy: 0.9616 - val_loss: 0.1097\n",
      "Epoch 20/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1571 - val_accuracy: 1.0000 - val_loss: 0.0493\n",
      "Epoch 21/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1013s\u001b[0m 2s/step - accuracy: 0.9704 - loss: 0.0937 - val_accuracy: 0.9274 - val_loss: 0.1749\n",
      "Epoch 22/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0525 - val_accuracy: 0.9600 - val_loss: 0.0682\n",
      "Epoch 23/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1014s\u001b[0m 2s/step - accuracy: 0.9877 - loss: 0.0618 - val_accuracy: 0.9502 - val_loss: 0.1245\n",
      "Epoch 24/24\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0517 - val_accuracy: 1.0000 - val_loss: 0.0862\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(224, 224),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary',\n",
    "                                                    subset='training')\n",
    "validation_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                         target_size=(224, 224),\n",
    "                                                         batch_size=32,\n",
    "                                                         class_mode='binary',\n",
    "                                                         subset='validation')\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "                    validation_steps=validation_generator.samples // validation_generator.batch_size)\n",
    "\n",
    "# Unfreeze some layers of the base model\n",
    "for layer in base_model.layers[-5:]:  # Unfreeze the last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # Lower learning rate\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "history_fine = model.fit(train_generator,\n",
    "                         validation_data=validation_generator,\n",
    "                         epochs=24,\n",
    "                         steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "                         validation_steps=validation_generator.samples // validation_generator.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('largerepochlargerdata.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.13265048]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = r'C:\\Users\\alisa\\Downloads\\FAKE\\target generated\\MULTIBANDMELGANLJ043-0145_gen.wav'\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Plot and save the waveform image\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y)\n",
    "plt.title('Waveform')\n",
    "waveform_image_path = 'workplz.png'\n",
    "plt.savefig(waveform_image_path)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the image\n",
    "img = image.load_img(waveform_image_path, target_size=(224,224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Normalize the image\n",
    "img_array /= 255.0\n",
    "\n",
    "# Expand dimensions to match the input shape of the model\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "model.predict(img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
